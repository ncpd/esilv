{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy import stats\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, recall_score\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA = principal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * PCA is an example of unsupervized method: we do not use any knowledge of any target variable.\n",
    "  * Generally speaking it is just an orthogonal projection of $n$-dimensional space to $q$-dimensional space with $n > q$.\n",
    "  * If $q = 1$, PCA is an ortogonal projection to a line ( = $1$-dimensional space).\n",
    "  * This line is selected so that the projected data have maximum possible sample variance.\n",
    "  * PCA **must be applied to centered data**, i.e. each feature data must be cetered so that the mean is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the idea\n",
    "  * We first generate two dimensional data $X \\in \\mathbb{R}^{n,2}$.\n",
    "  * We center them.\n",
    "  * We depict the projection for a selected line and compute the resulting (sample) variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating random data using 2 data sets both generated via 2-dimensional normal distribution\n",
    "mean1 = [12,-2]\n",
    "cov1 = np.matrix([[15,8],[8,8]])\n",
    "n1 = 20\n",
    "mean2 = [1,-2]\n",
    "cov2 = np.matrix([[15,8],[8,8]])\n",
    "n2 = 0\n",
    "data1 = pd.DataFrame(np.random.multivariate_normal(mean1, cov1, n1))\n",
    "data2 = pd.DataFrame(np.random.multivariate_normal(mean2, cov2, n2))\n",
    "data = pd.concat([data1, data2])\n",
    "data.columns = ['x', 'y']\n",
    "n = n1 + n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_projections(data, projection):\n",
    "    #normalize the projection vector\n",
    "    projection = [coord/math.sqrt((projection[0]**2 + projection[1]**2)) for coord in projection]\n",
    "    #number of data\n",
    "    n = data.shape[0]\n",
    "    # plotting the original data\n",
    "    ig, axs = plt.subplots(1,2, figsize=(14,7))\n",
    "    actual_mean = data.mean()\n",
    "    print('Data mean: ({0:.2f},{1:.2f})'.format(actual_mean[0],actual_mean[1]))\n",
    "    axs[0].plot(actual_mean[0], actual_mean[1],'ro')\n",
    "    axs[0].set_title('Original data')\n",
    "    data.plot.scatter('x', 'y', ax=axs[0])\n",
    "    \n",
    "    # centering data (!! important)\n",
    "    data = data - data.mean()\n",
    "    actual_mean = data.mean()\n",
    "    print('Data mean after centering: ({0:.2f},{1:.2f})'.format(actual_mean[0],actual_mean[1]))\n",
    "    plt.xlim(-10,10)\n",
    "    plt.ylim(-10,10)\n",
    "    # plotting the centered data\n",
    "    data.plot.scatter('x', 'y', ax=axs[1])\n",
    "    axs[1].set_title('Centered data')\n",
    "    # plotting the line given by the projection vector\n",
    "    axs[1].plot([-100*projection[0],100*projection[0]],[-100*projection[1], 100*projection[1]],'g-')\n",
    "    # getting the orthogonal projections of the data points to the line\n",
    "    coordinates = [np.dot(data.iloc[i,:], projection) for i in range(n)]\n",
    "    dots = np.matrix([[coordinates[i]*projection[0], coordinates[i]*projection[1]] for i in range(n)]).T\n",
    "    # plotting the projections\n",
    "    axs[1].plot(dots[0], dots[1],'ro')\n",
    "    # variance of projected data points\n",
    "    distances = pd.DataFrame([math.sqrt(dots[0,i]**2 + dots[1,i]**2) for i in range(n)])\n",
    "    print(\"Sample variance of projected data: {0:.2f}\".format(pd.DataFrame(coordinates).var()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = [0,1]\n",
    "plot_data_projections(data, projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection line given by the first principal component returned by PCA\n",
    "  * We use sklearn implementation.\n",
    "  * Compare the resulting variance of projected data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit_transform(data)\n",
    "print(\"Weights of components (eigenvalues):\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(\"Components (eigenvectors):\")\n",
    "print(pca.components_)\n",
    "plot_data_projections(data, pca.components_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematics of PCA (just a hint)\n",
    "\n",
    "Assume the data are stored in the matrix $X \\in \\mathbb{R}^{n,p}$ and that thay are centered, meaning that\n",
    "$$\n",
    "    \\frac{1}{n}\\sum^n_i X_{ik} = 0 \\quad \\text{for all } k = 0,1,2, \\ldots, p-1.\n",
    "$$\n",
    "Let $w \\in \\mathbb{R}^p$ be a unit vector defining the line on which we want to project the data points. Using some linear algebra knowledge we can prove that the coordinates of the projection of $i$th data point $x_i = (X_{i0}, \\ldots, X_{i(p-1)})$ is the dot product (in Czech \"skalární součin\") of $x_i$ and $w$ multiplied by the vector $w$, namely $y_i = (x_i, w)w$.\n",
    "Since the mean of these points is zero\n",
    "$$\n",
    "    \\frac{1}{n}\\sum^n_i y_i = \\frac{1}{n}\\sum^n_i (x_i, w) w = \\left( \\frac{1}{n}\\sum^n_i x_i, w \\right) w = 0\n",
    "$$\n",
    "the variance of these points projections is\n",
    "$$\n",
    "    \\frac{1}{n}\\sum^n_i ||y_i||^2 = \\frac{1}{n}\\sum^n_i (x_i, w)^2 w^Tw = \\{ w \\text{ is a unit vector} \\} = \\frac{1}{n}\\sum^n_i (x_i, w)^2 = \\frac{1}{n}(Xw)^T(Xw) = w^T\\frac{X^TX}{n}w.\n",
    "$$\n",
    "So, denoting $V = \\frac{X^TX}{n}$ (note that it is the covariance matrix of the centered data), we are looking for $w \\in \\mathbb{R}^p$ such that\n",
    "$$\n",
    "    w^TVw \\text{ is maximum possible} \\quad \\text{and} \\quad w^Tw = ||w||^2 = 1.\n",
    "$$\n",
    "\n",
    "In words, **we are looking for the projection line so that the variance of orthogonally projected points on this line is maximum possible**.\n",
    "\n",
    "Using the method of Lagrange multipliers for optimisation with constraints (see the MI-MPI course) we get that the maximum value is attained if $w$ is a unit eigenvector of $V$ corresponding to the greatest eigenvalue $\\lambda_0$ (all eigenvalues of a positive definite matrix are positive real numbers). Note that then $w^TVw = \\lambda$. \n",
    "\n",
    "This idea can be applied repetitively (with some details we omit here). All in all, the eigenvectors are called **principal components**, they are known to be mutually orthogonal. The first principal component is the one with the greatest eigenvector, the second principal component is the one with the second greatest eigenvector and so on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PCA to visualize multidimensional data\n",
    "  * Data with $p$ features can be understood as set of vectors in $p$-dimensional vecotr space $\\mathbb{R}^p$.\n",
    "  * First two pricipal components given by PCA define a 2-dimensional space a.k.a. a plane.\n",
    "  * If we project all data points on this plane, we get a 2D chart where the points are \"parted\" as much as possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating random data using p-dimensional normal distribution\n",
    "p = 10\n",
    "n = 100\n",
    "mean = np.random.rand(p)*10\n",
    "A = np.random.rand(p,p)\n",
    "offset = np.diag(np.full(p, 20))\n",
    "cov = A.T*A + offset\n",
    "data = pd.DataFrame(np.random.multivariate_normal(mean, cov, n))\n",
    "data = data - data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit_transform(data)\n",
    "print(\"Weights of components (eigenvalues):\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(\"Components (eigenvectors):\")\n",
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_data = pd.DataFrame(pca.transform(data))\n",
    "projected_data.columns = ['PCA1', 'PCA2']\n",
    "projected_data.plot.scatter('PCA1', 'PCA2')\n",
    "projected_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to check that what we have said above\n",
    "# the projection coordinate of the first data point shoul equal the dot product (w,x_0)\n",
    "# where w is the corresponding principal component\n",
    "PCA1 = (np.matrix(pca.components_[0])*np.matrix(data.loc[0,:]).T)[0,0]\n",
    "PCA2 = (np.matrix(pca.components_[1])*np.matrix(data.loc[0,:]).T)[0,0]\n",
    "PCA1sklearn = projected_data.iloc[0,0]\n",
    "PCA2sklearn = projected_data.iloc[0,1]\n",
    "print(\"Coordinate of the first data point projection using sklearn: {0:.5f}, {1:.5f}\".format(PCA1sklearn, PCA2sklearn))\n",
    "print(\"Coordinate of the first data point projection using direct computation: {0:.5f}, {1:.5f}\".format(PCA1, PCA2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PCA as a dimensionality reduction tool\n",
    "  * For high dimension data with a lot of features we can use the PCA to lower the number of features.\n",
    "  * More precisely: insted of $p$ features we use their projections to $q < p$ principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and centering\n",
    "# these are the cleaned data from tutorial 3 with indicator features dropped\n",
    "data = pd.read_csv('dataForPCA.csv')\n",
    "data = data.drop([data.columns[0],data.columns[1]], axis=1) # dropping the Id columns\n",
    "data = data - data.mean()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain, dtest = train_test_split(data, test_size=0.25, random_state=17)\n",
    "X = dtrain.drop(['SalePrice'], axis = 1, errors = 'ignore')\n",
    "y = dtrain.SalePrice\n",
    "Xtest = dtest.drop(['SalePrice'], axis = 1, errors = 'ignore')\n",
    "ytest = dtest.SalePrice\n",
    "\n",
    "# Linear Regression\n",
    "clf1 = LinearRegression()\n",
    "clf1.fit(X, y) \n",
    "\n",
    "# Print RMSLE\n",
    "RMSLE_OLS = np.sqrt(mean_squared_error(clf1.predict(Xtest), ytest))\n",
    "print('Root mean squared logarithmic error:', RMSLE_OLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 50 # number of components\n",
    "\n",
    "X = dtrain.drop(['SalePrice'], axis = 1, errors = 'ignore')\n",
    "y = dtrain.SalePrice\n",
    "Xtest = dtest.drop(['SalePrice'], axis = 1, errors = 'ignore')\n",
    "ytest = dtest.SalePrice\n",
    "\n",
    "#### \n",
    "# pca with and without scaling\n",
    "####\n",
    "pca = PCA()\n",
    "pca.fit_transform(X)\n",
    "X1 = pca.transform(X)\n",
    "Xtest1 = pca.transform(Xtest)\n",
    "\n",
    "pca.fit_transform(scale(X))\n",
    "X2 = pca.transform(scale(X))\n",
    "Xtest2 = pca.transform(scale(Xtest))\n",
    "RMSLE = []\n",
    "RMSLE_scale = []\n",
    "for n in range(1,q):\n",
    "    Xsub1 = X1[:,0:n]\n",
    "    Xsubtest1 = Xtest1[:,0:n]\n",
    "    clf1 = LinearRegression()\n",
    "    clf1.fit(Xsub1, y) \n",
    "    # save RMSLE\n",
    "    RMSLE.append(np.sqrt(mean_squared_error(clf1.predict(Xsubtest1), ytest)))\n",
    "    \n",
    "    Xsub2 = X2[:,0:n]\n",
    "    Xsubtest2 = Xtest2[:,0:n]\n",
    "    clf1 = LinearRegression()\n",
    "    clf1.fit(Xsub2, y) \n",
    "    # save RMSLE\n",
    "    RMSLE_scale.append(np.sqrt(mean_squared_error(clf1.predict(Xsubtest2), ytest)))\n",
    "plt.subplots(1,1, figsize=(15, 8))\n",
    "ns = plt.scatter(range(1,q), RMSLE, c='red')\n",
    "s = plt.scatter(range(1,q), RMSLE_scale, c='green')\n",
    "plt.title(u\"RMSLE as a function of number of principal components used\")\n",
    "plt.xlabel(u'number of principal component used')\n",
    "plt.ylabel('RMSLE')\n",
    "plt.plot([0, q], [RMSLE_OLS, RMSLE_OLS],'b-')\n",
    "plt.legend((ns,s),('non-scaled', 'scaled'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA = least discriminant analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * It is applicable on **classification data**, i.e. the target variable is cathegorical.\n",
    "  * It is again a projection to a lower-dimensional subspace.\n",
    "  * The get the idea assume we project data to a line and that we have two categories:\n",
    "    * Let $\\mu_1$ and $\\mu_2$ are means of the projected datapoints of the two catoegories, respectively.\n",
    "    * Similarly, assume $\\sigma^2_1$ and $\\sigma^2_2$ are the sample variances of the projected datapoints.\n",
    "    * The goal of the LDA is to find the line so that the means $\\mu_1$ and $\\mu_2$ are far away from each other but at the same time the sum of variances $\\sigma^2_1 + \\sigma^2_2$ is low.\n",
    "    * Details can be found below, in words we can say that LDA is looking for the projection maximizing **separability** of the data points from distinct categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection of 2D data to a line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating random data using 2 data sets both generated via 2-dimensional normal distribution\n",
    "mean1 = [12,-2]\n",
    "cov1 = np.matrix([[5,4],[4,8]])\n",
    "n1 = 20\n",
    "mean2 = [4,-4]\n",
    "cov2 = np.matrix([[10,4],[4,8]])\n",
    "n2 = 20\n",
    "data1 = pd.DataFrame(np.random.multivariate_normal(mean1, cov1, n1))\n",
    "data1['class'] = np.full(n1,0).T\n",
    "data2 = pd.DataFrame(np.random.multivariate_normal(mean2, cov2, n2))\n",
    "data2['class'] = np.full(n2,1).T\n",
    "data = pd.concat([data1, data2])\n",
    "data.columns = ['x', 'y','c']\n",
    "# centering the data for better looking plots\n",
    "data[['x','y']] = data[['x','y']] - data[['x','y']].mean()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_projections(data, projection):\n",
    "    mean = []\n",
    "    var = []\n",
    "    lim = 15\n",
    "    #normalize the projection vector\n",
    "    projection = [coord/math.sqrt((projection[0]**2 + projection[1]**2)) for coord in projection]\n",
    "    #number of data\n",
    "    n = data.shape[0]\n",
    "    # plotting the original data\n",
    "    mean_all = data[['x', 'y']].mean()\n",
    "    ig, ax = plt.subplots(1,1,figsize=(10, 10))\n",
    "    data[data['c'] == 1].plot.scatter('x', 'y', c = 'red', ax=ax)\n",
    "    data[data['c'] == 0].plot.scatter('x', 'y', c = 'blue', ax=ax)\n",
    "    plt.xlim(mean_all[0]-lim,mean_all[0] + lim)\n",
    "    plt.ylim(mean_all[1]-lim,mean_all[1] + lim)\n",
    "    \n",
    "    ax.plot([-10*lim*projection[0],10*lim*projection[0]],[-10*lim*projection[1], 10*lim*projection[1]],'g-')\n",
    "    # getting the orthogonal projections of the data points to the line\n",
    "    colors = ['b','r']\n",
    "    for k in [0,1]:\n",
    "        color = colors[k]\n",
    "        cat = data[data['c'] == k]\n",
    "        coordinates = [np.dot(cat.iloc[i,0:2], projection) for i in range(cat.shape[0])]\n",
    "        dots = np.matrix([[coordinates[i]*projection[0], coordinates[i]*projection[1]] for i in range(cat.shape[0])]).T\n",
    "        # plotting the projections\n",
    "        ax.plot(dots[0], dots[1], color + 'x')\n",
    "        # variance of projected data points\n",
    "        distances = pd.DataFrame(coordinates)\n",
    "        var.append(distances.var()[0])\n",
    "        mean.append(distances.mean()[0])\n",
    "    return mean, var\n",
    "mean, var = plot_data_projections(data,[1,2])\n",
    "print(\"Cathegories means: {0:.2f}, {1:.2f}\".format(mean[0], mean[1]))\n",
    "print(\"Cathegories sample variances: {0:.2f}, {1:.2f}\".format(var[0], var[1]))\n",
    "objective = (mean[0] - mean[1])**2/(var[0] + var[1])\n",
    "print(\"Objective function: {0:.2f}\".format(objective))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection line given by LDA\n",
    "  * We use sklearn implementation.\n",
    "  * Compare the resulting objective function value of projected data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearDiscriminantAnalysis(n_components=1,solver='svd', shrinkage=None).fit(data[['x','y']], data['c'])\n",
    "mean, var = plot_data_projections(data, clf.coef_[0])\n",
    "print(\"Cathegories means: {0:.2f}, {1:.2f}\".format(mean[0], mean[1]))\n",
    "print(\"Cathegories sample variances: {0:.2f}, {1:.2f}\".format(var[0], var[1]))\n",
    "objective = (mean[0] - mean[1])**2/(var[0] + var[1])\n",
    "print(\"Objective function: {0:.2f}\".format(objective))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematics of LDA (just a hint)\n",
    "\n",
    "Assume the data are stored in the matrix $X \\in \\mathbb{R}^{n,p}$ and that there are $K$ classes (cathegories) of data points. Let us denote $N_c$ the number of points on class $c$. The class is indicated in target variable $Y \\in \\mathbb{R}^{n,1}$. Let $\\mu \\in \\mathbb{R}^p$ be the mean vector of all data and $\\mu_c$ the mean vectors for class $c \\in \\{1,2, \\ldots, K\\}$.\n",
    "\n",
    "We define **scatter matrices** from $\\mathbb{R}^{p,p}$\n",
    "$$\n",
    "    S_B = \\sum_{c = 1}^K N_c (\\mu_c - \\mu)(\\mu_c - \\mu)^T\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    S_W = \\sum_{c = 1}^K \\sum_{i \\in \\text{class}_c} (x_i - \\mu_c)(x_i - \\mu_c)^T\n",
    "$$\n",
    "where by $i \\in \\text{class}_c$ we mean that $i$ is an index of a data point in class $c$.\n",
    "\n",
    "Let $w \\in \\mathbb{R}^{p}$ be a vector that define a projection line. Then LDA looks for $w$ that maximize the objective function\n",
    "$$\n",
    "    J(w) = \\frac{w^T S_B w}{w^T S_W w}.\n",
    "$$\n",
    "Again usin the method of Lagrange multipliers for optimisation with constraints we get that the (mutually orthogonal) local extremal values are attained if $w$ is an egenvector of the matrix $S_W^{-1}S_B$. The corresponding eigenvalue $\\lambda$ is again the value of the objective function, i.e., $J(w) = \\lambda$. Note that the rank of $S_B$ is at most $K - 1$ and hence the maximum number of nonzero eigenvalues is also at most $K - 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LDA to visualize multidimensional data\n",
    "  * We will use the famous iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "y_names = iris.target_names\n",
    "# pca\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "Xpca = pca.transform(X)\n",
    "print(Xpca.shape)\n",
    "# lda\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X, y)\n",
    "Xlda = lda.transform(X)\n",
    "print(Xlda.shape)\n",
    "\n",
    "\n",
    "ig, ax = plt.subplots(1,2,figsize=(14, 7))\n",
    "colors = ['red', 'blue', 'green']\n",
    "\n",
    "for i in [0, 1, 2]:\n",
    "    ax[0].scatter(Xlda[y == i, 0], Xlda[y == i, 1], color=colors[i], label=y_names[i])\n",
    "ax[0].legend()\n",
    "ax[0].set_title('LDA for IRIS dataset')\n",
    "ax[0].set_xlabel('LDA1')\n",
    "ax[0].set_ylabel('LDA2')\n",
    "\n",
    "for i in [0, 1, 2]:\n",
    "    ax[1].scatter(Xpca[y == i, 0], Xpca[y == i, 1], color=colors[i], label=y_names[i])\n",
    "ax[1].legend()\n",
    "ax[1].set_title('PCA for IRIS dataset')\n",
    "ax[1].set_xlabel('PCA1')\n",
    "ax[1].set_ylabel('PCA2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load digits data\n",
    "digits = datasets.load_digits()\n",
    "data = pd.DataFrame(digits.data)\n",
    "data['target'] = digits.target\n",
    "# print(digits.DESCR)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain, dtest = train_test_split(data, test_size=0.25, random_state=17)\n",
    "X = dtrain.drop(['target'], axis = 1, errors = 'ignore')\n",
    "y = dtrain.target\n",
    "Xtest = dtest.drop(['target'], axis = 1, errors = 'ignore')\n",
    "ytest = dtest.target\n",
    "\n",
    "# Logistic Regression\n",
    "clf1 = LogisticRegression()\n",
    "clf1.fit(X, y) \n",
    "\n",
    "# Print RMSLE\n",
    "recall_logres = recall_score(clf1.predict(Xtest), ytest, average='weighted')\n",
    "print('Recall for logistic regression: {0:.4f}'.format(recall_logres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=50)\n",
    "Xlda = lda.fit(X, y).transform(X)\n",
    "lda.predict(Xtest)\n",
    "recall_lda = recall_score(lda.predict(Xtest), ytest, average='weighted')\n",
    "print('Recall of LDA: {0:.4f}'.format(recall_lda))\n",
    "clf1 = LogisticRegression()\n",
    "clf1.fit(Xlda, y)\n",
    "recall_logres = recall_score(clf1.predict(lda.transform(Xtest)), ytest, average='weighted')\n",
    "print('Recall of Logistic regression with LDA: {0:.4f}'.format(recall_logres))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: plot the graph of how LogisticRegression recall depends on n_components parameter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
